Title:
<Enhancing NanoGPT via Squentropy Loss and Hyperparameter Tuning

Abstract:
<In the field of natural language processing, there is a focus to make large language models (LLM's) more compact while maintaining efficiency; NanoGPT is a promising model to address resource constraints without compromising performance significantly. There are questions surrounding the model: In what ways can the model's performance be improved? A few potential enhancements to NanoGPT's performance could be via (1) optimizing different loss functions and (2) hyperparameter tuning. Traditionally, most language models attempt to minimize the cross entropy loss function when predicting the next token in the sequence. However, this paper introduces a new solution in an attempt to improve performance: (1) optimize the \textit{squentropy} loss function and (2) choose optimal hyperparameters. Squentropy is a hybrid loss function combines the formulas of cross entropy loss and mean squared error loss. The implementation is difficult because the output tensors need to be manipulated and reshaped to be used for the correct squentropy calculation. The implementation is mathematically intensive, relying on vector algebra and probability theory. The mathematical steps were translated into python code. After implementing the changes and performing hyperparameter tuning (will be discussed in future section) and testing the model on various datasets, it seems as if squentropy loss could work almost as effectively as cross entropy loss. The new model produced effective results in terms of perplexity/loss, but did not beat the performance of the baseline cross entropy model (which seems to be the most effective>
